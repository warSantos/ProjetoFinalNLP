{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Avaliação de estratégias para melhora de classificação com word embeddings estáticos.\n",
        "\n",
        "Neste notebook apresenta-se a avaliação de diferentes estratégias de enriquecimento de embeddings estáticos para melhorar o desempenho de modelos na classificação de documentos. Neste notebook utilizaremos de recursos comumente utilizados em arquiteturas transformers, mais especificamente Positional Encoding e Self-Attention aplicados em menor escala para verificar o impacto destas estratégias na classificação de documentos. Para tal análise, utilizaremos a base de dados do jornal Folha de São Paulo disponível no link https://www.kaggle.com/marlesson/news-of-the-site-folhauol. Esta base conta com mais de 167 mil documentos e os atributos texto (conteúdo das notícias), title (título da notícia), category (e.g., esporte, colunas, mercado,...), subcategory (e.g., futebol), date (data de publicação) e link para notícia. Destes atributos utilizaremos o conteúdo de texto para produção de vetores densos o atributo category como rótulo para tarefa de classificação. Para construção de vetores densos estáticos utilizaremos o modelo Fasttext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZcsJKGqz3vew"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"10\" # export OMP_NUM_THREADS=4\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"10\" # export OPENBLAS_NUM_THREADS=4 \n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"10\" # export MKL_NUM_THREADS=6\n",
        "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"10\" # export VECLIB_MAXIMUM_THREADS=4\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"10\" # export NUMEXPR_NUM_THREADS=6\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import cld3\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import FastText\n",
        "from scipy.special import softmax\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9QUnqes5pEj"
      },
      "source": [
        "## Variáveis de controle de Execução\n",
        "* Para pré-processar novamente o texto basta alterar a variável PREPROCESS para True.\n",
        "* Para treinar novamente os modelos cbow e skip-gram basta alterar a variável TRAIN_EMBEDDINGS para True.\n",
        "* Para recalcular os pesos TF-IDF basta alterar a variável EVALUATE_TFIDF_WEIGHTS.\n",
        "* Para produzir novas representações vetoriais dos documentos basta alterar a variável MAKE_EMBEDDINGS para True.\n",
        "* Para executar o notebook com a função de debug (utiliza somente 100 instâncias. Função para teste) basta alterar a variável IS_TRAIN para True.\n",
        "* Para trabalhar com outras categorias basta alterar BASE_COLS aidcionando ou removendo categorias. Deve-se também, processar novamente o texto e produzir novos embeddings para que a produção dos embeddings não sofra com possíveis desencontros entre o vocabulário dos embeddings e o texto.\n",
        "* Para utilizar os scores (métricas já pré-computadas) basta configurar a variável LOAD_SCORES com valor True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yc4i8pul5ovA"
      },
      "outputs": [],
      "source": [
        "PREPROCESS = False\n",
        "TRAIN_EMBEDDINGS = False\n",
        "EVALUATE_TFIDF_WEIGHTS = False\n",
        "MAKE_EMBEDDINGS = True\n",
        "IS_TRAIN = False\n",
        "BASE_COLS = [\"colunas\", \"cotidiano\", \"esporte\", \"ilustrada\", \"mercado\", \"mundo\", \"poder\"]\n",
        "LOAD_SCORES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPT_OkK47R4k"
      },
      "source": [
        "## Pré-Processando texto\n",
        "Antes de produzir os vetores densos, aplicamos uma etapa de pré-processamento dos dados, constituída das seguintes sub-etapas: remoção de notícias em língua não portuguesa, remoção de documentos com menos de 15 palavras, conversão de texto para lowercase, remoção de stop_words e pontuação e tokenização. O pré-processamento só é aplicado se a variável PREPROCESS estiver com valor True. Caso contrário, valores padrões serão carregados (execução mais rápida)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QzHdJoZs7NdH"
      },
      "outputs": [],
      "source": [
        "def get_language(text):\n",
        "\n",
        "    return cld3.get_language(text)\n",
        "\n",
        "\n",
        "def get_stop_words():\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = {w: True for w in stopwords.words('portuguese')}\n",
        "    return stop_words\n",
        "\n",
        "\n",
        "def get_tokenizer():\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def clean_text(text, tokenizer, stop_words):\n",
        "    tokens = [word for word in tokenizer.tokenize(\n",
        "        text.lower()) if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def show_results(dict_scores, pattern=None):\n",
        "\n",
        "    table = []\n",
        "    for emb in dict_scores:\n",
        "        table.append([emb,\n",
        "                      np.round(np.mean(dict_scores[emb][\"precision\"])*100, decimals=2),\n",
        "                      np.std(dict_scores[emb][\"precision\"]),\n",
        "                      np.round(np.mean(dict_scores[emb][\"recall\"])*100, decimals=2),\n",
        "                      np.std(dict_scores[emb][\"recall\"]),\n",
        "                      np.round(np.mean(dict_scores[emb][\"micro\"])*100, decimals=2),\n",
        "                      np.std(dict_scores[emb][\"micro\"]),\n",
        "                      np.round(np.mean(dict_scores[emb][\"macro\"])*100, decimals=2),\n",
        "                      np.std(dict_scores[emb][\"macro\"])])\n",
        "\n",
        "    sdf = pd.DataFrame(table, columns=[\"Embedding\", \"Precision\", \"Std Precision\",\n",
        "                       \"Recall\", \"Std Recall\", \"Micro\", \"Std Micro\", \"Macro\", \"Std Macro\"])\n",
        "    if pattern is not None:\n",
        "        return sdf[~sdf.Embedding.str.contains(pattern)].sort_values(\n",
        "            by=[\"Macro\"], ascending=False)\n",
        "    else:\n",
        "        return sdf.sort_values(\n",
        "            by=[\"Macro\"], ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvB_U6e_77lD"
      },
      "outputs": [],
      "source": [
        "if PREPROCESS:\n",
        "  \n",
        "  data_path = f\"data/articles.csv\"\n",
        "  \n",
        "  # Filtrando colunas do dataset.\n",
        "  df = pd.read_csv(data_path).query(f\"category in {BASE_COLS}\")\n",
        "  df = df[df.text.notna()]\n",
        "  \n",
        "  # Filtrando mensagens que não estão em portugues.\n",
        "  lang_info = df.text.apply(lambda text: get_language(text))\n",
        "  lang = [ d[0] for d in lang_info ]\n",
        "  probs = [ d[1] for d in lang_info ]\n",
        "  df[\"lang\"] = lang\n",
        "  df[\"probs\"] = probs\n",
        "  df = df[df.lang == 'pt']\n",
        "  \n",
        "  # Filtrando stopwords.\n",
        "  tokenizer = get_tokenizer()\n",
        "  stop_words = get_stop_words()\n",
        "  df[\"text_clean\"] = df.text.apply(lambda text: clean_text(text, tokenizer, stop_words))\n",
        "  df.to_csv(f\"data/pt_clean_text.csv\", index=False)\n",
        "\n",
        "  sample = df.sample(frac=0.18, random_state=42)\n",
        "  sample.to_csv(f\"data/pt_sample.csv\", index=False)\n",
        "\n",
        "else:\n",
        "  df = pd.read_csv(f\"data/pt_clean_text.csv\")\n",
        "  sample = pd.read_csv(f\"data/pt_sample.csv\")\n",
        "  sample[\"sent_tokens\"] = sample.text_clean.apply(lambda text: text.split(' '))\n",
        "  sample[\"sent_len\"] = sample.sent_tokens.apply(lambda tokens: len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKnnkn-B30pW"
      },
      "source": [
        "## Treinando/Carregando os modelos de word-embeddings.\n",
        "\n",
        "Para produzir os vetores densos representantes do vocabulário, utilizamos o modelo Fasttext com os hiperparâmetros: dimensão dos vetores (vector_size) 100, tamanho da janela (window) 10, frequência mínima de aparições para palavras (min_count) 3, alpha 0.025 e número de épocas 10. Treinamos dois modelos, um com cbow e outro com skip-gram, ambos sobre o mesmo conjunto de hiperparâmetros. Para produzir novos embeddings a variável TRAIN_EMBEDDINGS deve estar com valor True. Caso contrário os modelos previamente treinados serão carregados (execução mais rápida)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ov5fonN6km-"
      },
      "outputs": [],
      "source": [
        "def train_model(texts, sg=1):\n",
        "\n",
        "  model = FastText(vector_size=100, window=10, min_count=3, alpha=0.025, min_alpha=0.025, workers=15, sg=sg)\n",
        "  model.build_vocab(texts)\n",
        "  for epoch in tqdm(range(10)):\n",
        "    model.train(texts, total_examples=model.corpus_count, epochs=1)\n",
        "    model.alpha -= 0.002\n",
        "    model.min_alpha = model.alpha\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9mn1tMa9f9d"
      },
      "outputs": [],
      "source": [
        "if TRAIN_EMBEDDINGS:\n",
        "  df = pd.read_csv(f\"data/pt_clean_text.csv\")\n",
        "  cbow_model = train_model(df.sent_tokens.values, sg=0)\n",
        "  skip_model = train_model(df.sent_tokens.values, sg=1)\n",
        "  cbow_model.save(f\"models/fasttext_cbow\")\n",
        "  skip_model.save(f\"models/fasttext_sg\")\n",
        "else:\n",
        "  cbow_model = FastText.load(\"models/fasttext_cbow\")\n",
        "  skip_model = FastText.load(\"models/fasttext_sg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGikWX8g4A8r"
      },
      "source": [
        "## Estatísticas dos documentos. Tamanho máximo, mínimo, médio e desvio padrão do tamanho dos documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "gg3ai6ja4Xqx",
        "outputId": "f47202a4-2ca4-4271-f8c6-bf009ab3546e"
      },
      "outputs": [],
      "source": [
        "sample.groupby(\"category\").agg({\"sent_len\": [\"max\", \"min\", \"mean\", \"std\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Os documentos não possuem padrão bem definido em relação a quantidade de tokens para qualquer uma das categorias, como podemos ver, para todas as categorias temos um elevado desvio (acima de 140 para todas as categorias) padrão na quantidade de tokens médio dos documentos. Neste trabalho, a produção de vetores com e sem o cálculo de atenção são independentes do tamanho dos documentos, porém, removemos documentos com tamanho inferior a 15 palavras, pois divergem da característica de uma notícia (geralmente documentos mais longos) e contribuem de forma mais negativa que positiva (ruidosa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLEDAeF84cMM"
      },
      "source": [
        "### Removendo documentos com menos de 15 palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C85uwxs4gIS"
      },
      "outputs": [],
      "source": [
        "sample = sample[sample.sent_len > 15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Definindo o tamanho da corpus para execução.\n",
        "Para execução rápida e depuração do notebook configure a variável IS_TRAIN com valor True e uma fração de 100 unidades dos dados de amostra será utilizada (execução rápida). Não é garantido desempenho consistente com esta implementação. Funcionalidade utilizada somente para fins de teste.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if IS_TRAIN:\n",
        "    SIZE = 100\n",
        "else:\n",
        "    SIZE = sample.shape[0]\n",
        "SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WChmYN4i4klA"
      },
      "source": [
        "### Distribuição de documentos pelas classes.\n",
        "Como mostrado no gráfico abaixo, as classes estão bem balanceadas, nenhuma das classes possui o dobro dos documentos da outra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "uOq3izhS4kVq",
        "outputId": "d7598210-abc3-46c9-9ae1-b12ac44dadff"
      },
      "outputs": [],
      "source": [
        "sample.category.value_counts().plot(kind='bar', grid=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4_GBZBZ4oz6"
      },
      "source": [
        "### Obtendo TF-IDF das palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = TfidfVectorizer(min_df=3)\n",
        "_ = tf.fit_transform(df.text_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_tfidf_norm(sent, cbow_model, tfidf):\n",
        "    x = np.array([ tfidf.idf_[tfidf.vocabulary_[token]] for token in sent if token in tfidf.vocabulary_ and token in cbow_model.wv ])\n",
        "    x = x / np.max(x)\n",
        "    return x\n",
        "if EVALUATE_TFIDF_WEIGHTS:\n",
        "    tfidf_matrix = np.array([ get_tfidf_norm(sent, cbow_model, tf) for sent in tqdm(sample.sent_tokens.values) ])\n",
        "    np.save(\"embeddings/tfidf_scores.npy\", tfidf_matrix, allow_pickle=True)\n",
        "else:\n",
        "    tfidf_matrix = np.load(\"embeddings/tfidf_scores.npy\", allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Sany774udw"
      },
      "source": [
        "## Representações vetoriais.\n",
        "Neste trabalho o nosso objetivo é investigar estratégias para produção de embeddings ricos em contexto, com menor custo computacional. Com inspiração nas camadas de self-attention utilizadas em arquiteturas Transformers, neste trabalho investigaremos a aplicação de self-attention em embeddings Fasttext. Nosso objetivo é, produzir para cada palavra em um documento qualquer em nosso corpus, embeddings contextualizados de cada palavra de modo a “saturar” a contextualização do embedding do documento através da média dos embeddings das palavras que o constitue. Com esta estratégia esperamos “amplificar” a informação contextual no embedding dos documentos. Desta maneira consideramos duas abordagens de self-attention, scaled product self-attention (SP-SA) e cosine self-attention. \n",
        "SP-SA é uma das formas de se calcular self-attention e é utilizada em modelos como Transformers e BERT. Para enriquecer os embeddings de palavras de um documento $W$ com o contexto formado por palavras vizinhas estima-se o “score” de importância entre os para todo par de palavra $wi, wj \\in W$ através do produto escalar entre os embeddings que as representam. Logo, temos o score entre duas palavras definido como:\n",
        "\n",
        "$score(w_i, w_j) = e_i \\times e_j$\n",
        "\n",
        "Sendo a matriz $A$ com os scores de atenção, definida como: \n",
        "\n",
        "$A^{W \\times W}, \\quad A_{ij} = score(w_i,w_j)$\n",
        "\n",
        "Após a construção de A, normalizamos os scores com aplicando a função softmax:\n",
        "\n",
        "$Att = softmax(A), \\quad Att^{W \\times W}$\n",
        "\n",
        "Após obter a matriz $Att$ contextualizamos os vetores das palavras multiplicando os embeddings E do documento $W$, pela matriz $Att$, resultando na matrix $C$.\n",
        "\n",
        "$C^{W \\times d}, \\quad C = E^{T} \\cdot Att, \\quad E^{W \\times d}$, sendo $d$ a dimensão dos vetores densos Fasttext.\n",
        "\n",
        "Cada vetor $ci$ de $C$, consiste no vetor contextualizado da palavra $wi$. O vetor resultante do documento é dado como a média de $C$.\n",
        "De forma similar, para atenção com base na similaridade de cosseno, apenas alteramos a função de score que agora é definida como se segue:\n",
        "\n",
        "$score(w_i,w_j) = cosseno(e_i,e_j)$\n",
        "\n",
        "Além da aplicação de atenção, analisamos também o impacto da adição de posição (Vaswani et. al., 2017) e a ponderação dos embeddings das palavras a partir do TF-IDF das palavras sobre o corpus de notícias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUH850Hh4uAp"
      },
      "outputs": [],
      "source": [
        "def get_positional_embs(n_words=10000, dim=200):\n",
        "\n",
        "    return np.vstack([positional_vector(i, dim=dim) for i in range(n_words)])\n",
        "\n",
        "\n",
        "def positional_vector(pos, dim=100, denom=10000):\n",
        "\n",
        "    pos_values = []\n",
        "    for i in range(dim):\n",
        "        pw = 2 * i\n",
        "        if i % 2 == 0:\n",
        "            res = np.sin(pos / np.power(denom, pw/dim))\n",
        "        else:\n",
        "            res = np.cos(pos / np.power(denom, pw/dim))\n",
        "        pos_values.append(res)\n",
        "    return np.array(pos_values)\n",
        "\n",
        "\n",
        "def check_word(word, cbow_model, skip_model, tf=None):\n",
        "\n",
        "    # Se a palavra estiver no cbow e no skipgram.\n",
        "    if word in cbow_model.wv and word in skip_model.wv:\n",
        "        # Se não estiver usando IDF.\n",
        "        if tf is None:\n",
        "            return True\n",
        "        else:\n",
        "            # Se a palavra estiver no vocabulário IDF.\n",
        "            if word in tf.vocabulary_:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def tokens_to_embs(sent_tokens, cbow, skip, option=1, pos_embs=None, idf=None, tf=None):\n",
        "\n",
        "    if option not in [1, 2, 3]:\n",
        "        option = 2\n",
        "\n",
        "    # Cbow somente\n",
        "    if option == 1:\n",
        "        words_vec = np.vstack([cbow.wv[token] for token in sent_tokens if check_word(\n",
        "            token, cbow_model, skip_model, tf)])\n",
        "    # Skip Gram\n",
        "    elif option == 2:\n",
        "        words_vec = np.vstack([cbow.wv[token] for token in sent_tokens if check_word(\n",
        "            token, cbow_model, skip_model, tf)])\n",
        "    # Skip e Cbow Combinados.\n",
        "    elif option == 3:\n",
        "        words_vec = np.vstack([np.hstack([cbow.wv[token], skip.wv[token]])\n",
        "                              for token in sent_tokens if check_word(token, cbow_model, skip_model, tf)])\n",
        "\n",
        "    if pos_embs is not None:\n",
        "        words_vec += pos_embs[:words_vec.shape[0], :words_vec.shape[1]]\n",
        "        return words_vec\n",
        "    if idf is not None:\n",
        "        return (words_vec.T * idf).T\n",
        "    return words_vec\n",
        "\n",
        "\n",
        "def get_raw_embs(sentences_tokes, cbow, skip, option=1, pos_embs=None, tfidf_matrix=None, tf=None):\n",
        "\n",
        "    if tfidf_matrix is None:\n",
        "        return np.vstack([np.mean(tokens_to_embs(sent_tokens, cbow, skip, option=option, pos_embs=pos_embs, tf=tf), axis=0) for sent_tokens in sentences_tokes])\n",
        "    else:\n",
        "        return np.vstack([np.mean(tokens_to_embs(sent_tokens, cbow, skip, option=option, pos_embs=pos_embs, idf=widf, tf=tf), axis=0) for sent_tokens, widf in zip(sentences_tokes, tfidf_matrix)])\n",
        "\n",
        "\n",
        "def attention_eval(embs, norm=\"mean\", att_type=\"product\"):\n",
        "\n",
        "    if att_type == \"product\":\n",
        "        w = softmax(np.inner(embs, embs)/np.sqrt(embs.shape[1]), axis=0)\n",
        "    else:\n",
        "        w = softmax(cosine_similarity(embs, embs), axis=0)\n",
        "\n",
        "    context_words = np.dot(w, embs)\n",
        "\n",
        "    if norm == \"mean\":\n",
        "        att = np.mean(context_words, axis=0)\n",
        "        return att\n",
        "    else:\n",
        "        att = softmax(np.sum(context_words, axis=0), axis=0)\n",
        "        return att\n",
        "\n",
        "\n",
        "def attention(sentences_tokes, cbow, skip, option=1, norm=\"mean\", pos_embs=None, tfidf_matrix=None, tf=None, att_type=\"product\"):\n",
        "\n",
        "    if tfidf_matrix is None:\n",
        "        return np.array([attention_eval(tokens_to_embs(sent_tokens, cbow, skip, option=option, pos_embs=pos_embs, tf=tf), att_type=att_type) for sent_tokens in sentences_tokes])\n",
        "    else:\n",
        "        return np.array([attention_eval(tokens_to_embs(sent_tokens, cbow, skip, option=option, pos_embs=pos_embs, idf=widf, tf=tf), att_type=att_type) for sent_tokens, widf in zip(sentences_tokes, tfidf_matrix)])\n",
        "\n",
        "\n",
        "def save_embs(dict_embs, embs_dir):\n",
        "\n",
        "    for emb in dict_embs:\n",
        "        np.save(f\"{embs_dir}/{emb}.npy\", dict_embs[emb], allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoFXymg34zMt"
      },
      "source": [
        "## Cross-Validation pipeline\n",
        "Para avaliar as representações de atenção, utilizaremos validação cruzada com 5 folds. Para avaliar o modelo, utilizamos as seguintes métricas: \n",
        "\n",
        "$precision = \\frac{TP_t}{TP_t + FP_t}$\n",
        "\n",
        "$recall = \\frac{TP_t}{TP_t + FN_t}$\n",
        "\n",
        "$MicroF1 = \\frac{2 \\cdot precision \\cdot  recall}{precision + recall}$\n",
        "\n",
        "$MacroF1 = \\frac{1}{|T|} \\cdot \\sum_{t \\in T} \\frac{2 \\cdot precision \\cdot  recall}{precision + recall}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RemI_Rnk42ej"
      },
      "outputs": [],
      "source": [
        "def evaluate(dict_embs, y_true, dict_scores, kf):\n",
        "\n",
        "    for key_emb in dict_embs:\n",
        "        print(f\"Embedding: {key_emb}\")\n",
        "        dict_scores[key_emb] = {}\n",
        "        fold = 0\n",
        "        # Para cada fold.\n",
        "        for train_idx, test_idx in kf.split(dict_embs[key_emb], y_true):\n",
        "            print(f\"\\tFold: {fold}\")\n",
        "            # Para cada classificador.    \n",
        "            logistic = LogisticRegression(max_iter=5000, n_jobs=10)\n",
        "            logistic.fit(dict_embs[key_emb][train_idx], y_true[train_idx])\n",
        "            y_pred = logistic.predict(dict_embs[key_emb][test_idx])\n",
        "            # Calculando métricas.\n",
        "            if \"micro\" not in dict_scores[key_emb]:\n",
        "                dict_scores[key_emb][\"micro\"] = []\n",
        "            dict_scores[key_emb][\"micro\"].append(f1_score(y_true[test_idx], y_pred, average=\"micro\"))\n",
        "            if \"macro\" not in dict_scores[key_emb]:\n",
        "                dict_scores[key_emb][\"macro\"] = []\n",
        "            dict_scores[key_emb][\"macro\"].append(f1_score(y_true[test_idx], y_pred, average=\"macro\"))\n",
        "            if \"precision\" not in dict_scores[key_emb]:\n",
        "                dict_scores[key_emb][\"precision\"] = []\n",
        "            dict_scores[key_emb][\"precision\"].append(precision_score(y_true[test_idx], y_pred, average=\"macro\", zero_division=0))\n",
        "            if \"recall\" not in dict_scores[key_emb]:\n",
        "                dict_scores[key_emb][\"recall\"] = []\n",
        "            dict_scores[key_emb][\"recall\"].append(recall_score(y_true[test_idx], y_pred, average=\"macro\"))\n",
        "            if \"y_pred\" not in dict_scores[key_emb]:\n",
        "                dict_scores[key_emb][\"y_pred\"] = {}\n",
        "            dict_scores[key_emb][\"y_pred\"][fold] = y_pred.tolist()\n",
        "            fold += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KaNK7jQ49nR"
      },
      "source": [
        "## Generating Baseline Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "RQCOs-hy471d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cbow: Done.\n",
            "skip: Done.\n",
            "cbow_skip: Done.\n",
            "cbow-tf-idf: Done.\n",
            "skip-tf-idf: Done.\n",
            "cbow_skip-tf-idf: Done.\n"
          ]
        }
      ],
      "source": [
        "dict_embs = {}\n",
        "if MAKE_EMBEDDINGS:\n",
        "    dict_embs[\"cbow\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=1)\n",
        "    print(\"cbow: Done.\")\n",
        "    dict_embs[\"skip\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=2)\n",
        "    print(\"skip: Done.\")\n",
        "    dict_embs[\"cbow_skip\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3)\n",
        "    print(\"cbow_skip: Done.\")\n",
        "    dict_embs[\"cbow-tf-idf\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=1, tfidf_matrix=tfidf_matrix, tf=tf)\n",
        "    print(\"cbow-tf-idf: Done.\")\n",
        "    dict_embs[\"skip-tf-idf\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=2, tfidf_matrix=tfidf_matrix, tf=tf)\n",
        "    print(\"skip-tf-idf: Done.\")\n",
        "    dict_embs[\"cbow_skip-tf-idf\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, tfidf_matrix=tfidf_matrix, tf=tf)\n",
        "    print(\"cbow_skip-tf-idf: Done.\")\n",
        "    save_embs(dict_embs, \"embeddings\")\n",
        "else:\n",
        "    dict_embs[\"cbow\"] = np.load(\"embeddings/cbow.npy\")\n",
        "    dict_embs[\"skip\"] = np.load(\"embeddings/skip.npy\")\n",
        "    dict_embs[\"cbow_skip\"] = np.load(\"embeddings/cbow_skip.npy\")\n",
        "    dict_embs[\"cbow-tf-idf\"] = np.load(\"embeddings/cbow-tf-idf.npy\")\n",
        "    dict_embs[\"skip-tf-idf\"] = np.load(\"embeddings/skip-tf-idf.npy\")\n",
        "    dict_embs[\"cbow_skip-tf-idf\"] = np.load(\"embeddings/cbow_skip-tf-idf.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w86SWl_Q5C24"
      },
      "source": [
        "# Avaliação das baselines\n",
        "\n",
        "Neste trabalho utilizaremos como baseline para as avaliações aqui feitas as seguintes representações: Fasttext-Cbow, Fasttext-Skip-Gram e Cbow-Skip-Gram (concatenação dos vetores duas estratégias).\n",
        "\n",
        "* cbow - Representações Cbow (Fasttext) sem alteraçães.\n",
        "* skip - Representações Skip-Gram (Fasttext) sem alteraçães.\n",
        "* cbow_skip - Combinação (Concatenação) do Skip-Gram com o Cbow.\n",
        "* cbow-tf-idf - Ponderação do embedding Cbow com o valor de TF-IDF da palavra.\n",
        "* skip-tf-idf - Ponderação do embedding Skip-Gram com o valor de TF-IDF da palavra.\n",
        "* cbow_skip-tf-idf - Ponderação da combinação Cbow-Skip com o valor de TF-IDF da palavra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "W9-k1rwH5HCg"
      },
      "outputs": [],
      "source": [
        "dict_scores = {}\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(sample.category.values[:SIZE])\n",
        "y_true = le.transform(sample.category.values[:SIZE])\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding: cbow\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: skip\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow-tf-idf\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: skip-tf-idf\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-tf-idf\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n"
          ]
        }
      ],
      "source": [
        "evaluate(dict_embs, y_true, dict_scores, kf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kkSNfBr25MlY"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Std Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Std Recall</th>\n",
              "      <th>Micro</th>\n",
              "      <th>Std Micro</th>\n",
              "      <th>Macro</th>\n",
              "      <th>Std Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cbow_skip</td>\n",
              "      <td>83.92</td>\n",
              "      <td>0.002951</td>\n",
              "      <td>84.24</td>\n",
              "      <td>0.002234</td>\n",
              "      <td>83.76</td>\n",
              "      <td>0.002540</td>\n",
              "      <td>84.05</td>\n",
              "      <td>0.002564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cbow_skip-tf-idf</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.002989</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003698</td>\n",
              "      <td>83.44</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cbow-tf-idf</td>\n",
              "      <td>83.11</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>83.28</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>skip-tf-idf</td>\n",
              "      <td>83.11</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>83.28</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cbow</td>\n",
              "      <td>83.09</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.002952</td>\n",
              "      <td>83.23</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>skip</td>\n",
              "      <td>83.09</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.002952</td>\n",
              "      <td>83.23</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Embedding  Precision  Std Precision  Recall  Std Recall  Micro  \\\n",
              "2         cbow_skip      83.92       0.002951   84.24    0.002234  83.76   \n",
              "5  cbow_skip-tf-idf      83.54       0.002989   83.97    0.003698  83.44   \n",
              "3       cbow-tf-idf      83.11       0.003136   83.54    0.003820  83.00   \n",
              "4       skip-tf-idf      83.11       0.003136   83.54    0.003820  83.00   \n",
              "0              cbow      83.09       0.003158   83.43    0.003236  82.93   \n",
              "1              skip      83.09       0.003158   83.43    0.003236  82.93   \n",
              "\n",
              "   Std Micro  Macro  Std Macro  \n",
              "2   0.002540  84.05   0.002564  \n",
              "5   0.003544  83.71   0.003160  \n",
              "3   0.003366  83.28   0.003314  \n",
              "4   0.003366  83.28   0.003314  \n",
              "0   0.002952  83.23   0.003043  \n",
              "1   0.002952  83.23   0.003043  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOAD_SCORES:\n",
        "    with open(\"outputs/scores.json\", 'r') as fd:\n",
        "        dict_scores = json.load(fd)\n",
        "\n",
        "show_results(dict_scores, pattern=\"positional|cosine|product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Desempenho baselines\n",
        "As baselines possuem resultados bem próximos entre si, com menos de 1% de diferença em Macro-F1 entre a melhor (cbow_skip) e pior representação (skip). As representações com cbow e skip gram se saíram ligeiramente melhor conforme esperado, uma vez que ambos modelos Cbow e Skip-Gram capturam informações diferentes, logo a combinação entre Cbow e Skip-Gram traz consigo um conjunto mais diverso de informação.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variantes\n",
        "\n",
        "* cbow_skip-product - Embeddings Cbow-Skip contextualizados com Scaled-Product-Attention.\n",
        "* cbow_skip-product-positional - Embeddings Cbow-Skip contextualizados com Scaled-Product-Attention mais informação de posição da palavra.\n",
        "* cbow_skip-tf-idf-product - Embeddings Cbow-Skip contextualizados com Scaled-Product-Attention mais ponderamento com TF-IDF.\n",
        "* cbow_skip-tf-idf-product-positional - Embeddings Cbow-Skip contextualizados com Scaled-Product-Attention mais ponderamento do TF-IDF e informação de posição da palavra.\n",
        "* cbow_skip-cosine - Embeddings Cbow-Skip contextualizados com Cosine-Attention.\n",
        "* cbow_skip-cosine-positional - Embeddings Cbow-Skip contextualizados com Cosine-Attention mais informação de posição da palavra.\n",
        "* cbow_skip-tf-idf-cosine - Embeddings Cbow-Skip contextualizados com Cosine-Attention mais ponderamento com TF-IDF.\n",
        "* cbow_skip-tf-idf-cosine-positional - Embeddings Cbow-Skip contextualizados com Cosine-Attention mais ponderamento do TF-IDF e informação de posição da palavra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pos_embs = get_positional_embs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cbow_skip-product: Done.\n",
            "cbow_skip-product-positional: Done.\n",
            "cbow_skip-tf-idf-product: Done.\n",
            "cbow_skip-tf-idf-product-positional: Done.\n",
            "cbow_skip-consise: Done.\n",
            "cbow_skip-consise-positional: Done.\n",
            "cbow_skip-tf-idf-consise: Done.\n",
            "cbow_skip-tf-idf-consise-positional: Done.\n"
          ]
        }
      ],
      "source": [
        "dict_embs = {}\n",
        "if MAKE_EMBEDDINGS:\n",
        "    # Product Attention.\n",
        "    dict_embs[\"cbow_skip-product\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3)\n",
        "    print(\"cbow_skip-product: Done.\")\n",
        "    dict_embs[\"cbow_skip-product-positional\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3)\n",
        "    print(\"cbow_skip-product-positional: Done.\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-product\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, tfidf_matrix=tfidf_matrix, tf=tf)\n",
        "    print(\"cbow_skip-tf-idf-product: Done.\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-product-positional\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, pos_embs=pos_embs, tfidf_matrix=tfidf_matrix, tf=tf)\n",
        "    print(\"cbow_skip-tf-idf-product-positional: Done.\")\n",
        "    # Consine Attention\n",
        "    dict_embs[\"cbow_skip-cosine\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, att_type=\"cosine\")\n",
        "    print(\"cbow_skip-cosine: Done.\")\n",
        "    dict_embs[\"cbow_skip-cosine-positional\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, att_type=\"cosine\")\n",
        "    print(\"cbow_skip-cosine-positional: Done.\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-cosine\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, tfidf_matrix=tfidf_matrix, att_type=\"cosine\", tf=tf)\n",
        "    print(\"cbow_skip-tf-idf-cosine: Done.\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-cosine-positional\"] = attention(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, pos_embs=pos_embs, tfidf_matrix=tfidf_matrix, att_type=\"cosine\", tf=tf)\n",
        "    print(\"cbow_skip-tf-idf-cosine-positional: Done.\")\n",
        "    save_embs(dict_embs, \"embeddings\")\n",
        "else:\n",
        "    # Product Attention\n",
        "    dict_embs[\"cbow_skip-product\"] = np.load(\"embeddings/cbow_skip-product.npy\")\n",
        "    dict_embs[\"cbow_skip-product-positional\"] = np.load(\"embeddings/cbow_skip-product-positional.npy\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-product\"] = np.load(\"embeddings/cbow_skip-tf-idf-product.npy\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-product-positional\"] = np.load(\"embeddings/cbow_skip-tf-idf-product-positional.npy\")\n",
        "    # Cosine Attention\n",
        "    dict_embs[\"cbow_skip-cosine\"] = np.load(\"embeddings/cbow_skip-cosine.npy\")\n",
        "    dict_embs[\"cbow_skip-cosine-positional\"] = np.load(\"embeddings/cbow_skip-cosine-positional.npy\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-cosine\"] = np.load(\"embeddings/cbow_skip-tf-idf-cosine.npy\")\n",
        "    dict_embs[\"cbow_skip-tf-idf-cosine-positional\"] = np.load(\"embeddings/cbow_skip-tf-idf-cosine-positional.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding: cbow_skip-product\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-product-positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-tf-idf-product\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-tf-idf-product-positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-consise\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-consise-positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-tf-idf-consise\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip-tf-idf-consise-positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n"
          ]
        }
      ],
      "source": [
        "evaluate(dict_embs, y_true, dict_scores, kf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Std Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Std Recall</th>\n",
              "      <th>Micro</th>\n",
              "      <th>Std Micro</th>\n",
              "      <th>Macro</th>\n",
              "      <th>Std Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>cbow_skip-cosine</td>\n",
              "      <td>83.93</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>84.25</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>84.06</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cbow_skip</td>\n",
              "      <td>83.92</td>\n",
              "      <td>0.002951</td>\n",
              "      <td>84.24</td>\n",
              "      <td>0.002234</td>\n",
              "      <td>83.76</td>\n",
              "      <td>0.002540</td>\n",
              "      <td>84.05</td>\n",
              "      <td>0.002564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cbow_skip-product</td>\n",
              "      <td>83.92</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>84.25</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.002616</td>\n",
              "      <td>84.05</td>\n",
              "      <td>0.002696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cbow_skip-tf-idf</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.002989</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003698</td>\n",
              "      <td>83.44</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cbow_skip-tf-idf-product</td>\n",
              "      <td>83.53</td>\n",
              "      <td>0.003039</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003578</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>cbow_skip-tf-idf-cosine</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003010</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003720</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003560</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cbow-tf-idf</td>\n",
              "      <td>83.11</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>83.28</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>skip-tf-idf</td>\n",
              "      <td>83.11</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>83.28</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cbow</td>\n",
              "      <td>83.09</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.002952</td>\n",
              "      <td>83.23</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>skip</td>\n",
              "      <td>83.09</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.002952</td>\n",
              "      <td>83.23</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Embedding  Precision  Std Precision  Recall  Std Recall  \\\n",
              "10          cbow_skip-cosine      83.93       0.002864   84.25    0.002213   \n",
              "2                  cbow_skip      83.92       0.002951   84.24    0.002234   \n",
              "6          cbow_skip-product      83.92       0.003059   84.25    0.002405   \n",
              "5           cbow_skip-tf-idf      83.54       0.002989   83.97    0.003698   \n",
              "8   cbow_skip-tf-idf-product      83.53       0.003039   83.97    0.003744   \n",
              "12   cbow_skip-tf-idf-cosine      83.54       0.003010   83.97    0.003720   \n",
              "3                cbow-tf-idf      83.11       0.003136   83.54    0.003820   \n",
              "4                skip-tf-idf      83.11       0.003136   83.54    0.003820   \n",
              "0                       cbow      83.09       0.003158   83.43    0.003236   \n",
              "1                       skip      83.09       0.003158   83.43    0.003236   \n",
              "\n",
              "    Micro  Std Micro  Macro  Std Macro  \n",
              "10  83.77   0.002427  84.06   0.002500  \n",
              "2   83.76   0.002540  84.05   0.002564  \n",
              "6   83.77   0.002616  84.05   0.002696  \n",
              "5   83.44   0.003544  83.71   0.003160  \n",
              "8   83.43   0.003578  83.71   0.003206  \n",
              "12  83.43   0.003560  83.71   0.003181  \n",
              "3   83.00   0.003366  83.28   0.003314  \n",
              "4   83.00   0.003366  83.28   0.003314  \n",
              "0   82.93   0.002952  83.23   0.003043  \n",
              "1   82.93   0.002952  83.23   0.003043  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOAD_SCORES:\n",
        "    with open(\"outputs/scores.json\", 'r') as fd:\n",
        "        dict_scores = json.load(fd)\n",
        "\n",
        "show_results(dict_scores, pattern=\"positional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7wunAZP5S3R"
      },
      "source": [
        "## Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "q9qZSVvA5SIJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cbow_positional: Done.\n",
            "skip_positional: Done.\n",
            "cbow_skip_positional: Done.\n"
          ]
        }
      ],
      "source": [
        "dict_embs = {}\n",
        "if MAKE_EMBEDDINGS:\n",
        "    dict_embs[\"cbow_positional\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=1, pos_embs=pos_embs, tfidf_matrix=tfidf_matrix)\n",
        "    print(\"cbow_positional: Done.\")\n",
        "    dict_embs[\"skip_positional\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=2, pos_embs=pos_embs, tfidf_matrix=tfidf_matrix)\n",
        "    print(\"skip_positional: Done.\")\n",
        "    dict_embs[\"cbow_skip_positional\"] = get_raw_embs(sample.sent_tokens.values[:SIZE], cbow_model, skip_model, option=3, pos_embs=pos_embs, tfidf_matrix=tfidf_matrix)\n",
        "    print(\"cbow_skip_positional: Done.\")\n",
        "    save_embs(dict_embs, \"embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding: cbow_positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: skip_positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n",
            "Embedding: cbow_skip_positional\n",
            "\tFold: 0\n",
            "\tFold: 1\n",
            "\tFold: 2\n",
            "\tFold: 3\n",
            "\tFold: 4\n"
          ]
        }
      ],
      "source": [
        "evaluate(dict_embs, y_true, dict_scores, kf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Std Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>Std Recall</th>\n",
              "      <th>Micro</th>\n",
              "      <th>Std Micro</th>\n",
              "      <th>Macro</th>\n",
              "      <th>Std Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>cbow_skip_positional</td>\n",
              "      <td>84.18</td>\n",
              "      <td>0.002251</td>\n",
              "      <td>84.48</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>84.02</td>\n",
              "      <td>0.002208</td>\n",
              "      <td>84.30</td>\n",
              "      <td>0.002306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>cbow_skip-tf-idf-product-positional</td>\n",
              "      <td>84.15</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>84.47</td>\n",
              "      <td>0.003615</td>\n",
              "      <td>83.99</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>84.28</td>\n",
              "      <td>0.003494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>cbow_skip-tf-idf-cosine-positional</td>\n",
              "      <td>84.15</td>\n",
              "      <td>0.003582</td>\n",
              "      <td>84.47</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>84.00</td>\n",
              "      <td>0.003330</td>\n",
              "      <td>84.28</td>\n",
              "      <td>0.003497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>cbow_skip-cosine-positional</td>\n",
              "      <td>83.93</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>84.25</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>84.06</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>cbow_skip-cosine</td>\n",
              "      <td>83.93</td>\n",
              "      <td>0.002864</td>\n",
              "      <td>84.25</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>84.06</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cbow_skip</td>\n",
              "      <td>83.92</td>\n",
              "      <td>0.002951</td>\n",
              "      <td>84.24</td>\n",
              "      <td>0.002234</td>\n",
              "      <td>83.76</td>\n",
              "      <td>0.002540</td>\n",
              "      <td>84.05</td>\n",
              "      <td>0.002564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cbow_skip-product</td>\n",
              "      <td>83.92</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>84.25</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.002616</td>\n",
              "      <td>84.05</td>\n",
              "      <td>0.002696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>cbow_skip-product-positional</td>\n",
              "      <td>83.92</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>84.25</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.002616</td>\n",
              "      <td>84.05</td>\n",
              "      <td>0.002696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>cbow_skip-tf-idf-cosine</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003010</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003720</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003560</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>cbow_skip-tf-idf-product</td>\n",
              "      <td>83.53</td>\n",
              "      <td>0.003039</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003744</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003578</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cbow_skip-tf-idf</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.002989</td>\n",
              "      <td>83.97</td>\n",
              "      <td>0.003698</td>\n",
              "      <td>83.44</td>\n",
              "      <td>0.003544</td>\n",
              "      <td>83.71</td>\n",
              "      <td>0.003160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>skip-tf-idf</td>\n",
              "      <td>83.11</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>83.28</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cbow-tf-idf</td>\n",
              "      <td>83.11</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>83.54</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>83.28</td>\n",
              "      <td>0.003314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>skip</td>\n",
              "      <td>83.09</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.002952</td>\n",
              "      <td>83.23</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cbow</td>\n",
              "      <td>83.09</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>83.43</td>\n",
              "      <td>0.003236</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.002952</td>\n",
              "      <td>83.23</td>\n",
              "      <td>0.003043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>cbow_positional</td>\n",
              "      <td>83.07</td>\n",
              "      <td>0.001461</td>\n",
              "      <td>83.42</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>83.21</td>\n",
              "      <td>0.001586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>skip_positional</td>\n",
              "      <td>83.07</td>\n",
              "      <td>0.001461</td>\n",
              "      <td>83.42</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>82.93</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>83.21</td>\n",
              "      <td>0.001586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Embedding  Precision  Std Precision  Recall  \\\n",
              "16                 cbow_skip_positional      84.18       0.002251   84.48   \n",
              "9   cbow_skip-tf-idf-product-positional      84.15       0.003577   84.47   \n",
              "13   cbow_skip-tf-idf-cosine-positional      84.15       0.003582   84.47   \n",
              "11          cbow_skip-cosine-positional      83.93       0.002864   84.25   \n",
              "10                     cbow_skip-cosine      83.93       0.002864   84.25   \n",
              "2                             cbow_skip      83.92       0.002951   84.24   \n",
              "6                     cbow_skip-product      83.92       0.003059   84.25   \n",
              "7          cbow_skip-product-positional      83.92       0.003059   84.25   \n",
              "12              cbow_skip-tf-idf-cosine      83.54       0.003010   83.97   \n",
              "8              cbow_skip-tf-idf-product      83.53       0.003039   83.97   \n",
              "5                      cbow_skip-tf-idf      83.54       0.002989   83.97   \n",
              "4                           skip-tf-idf      83.11       0.003136   83.54   \n",
              "3                           cbow-tf-idf      83.11       0.003136   83.54   \n",
              "1                                  skip      83.09       0.003158   83.43   \n",
              "0                                  cbow      83.09       0.003158   83.43   \n",
              "14                      cbow_positional      83.07       0.001461   83.42   \n",
              "15                      skip_positional      83.07       0.001461   83.42   \n",
              "\n",
              "    Std Recall  Micro  Std Micro  Macro  Std Macro  \n",
              "16    0.002693  84.02   0.002208  84.30   0.002306  \n",
              "9     0.003615  83.99   0.003324  84.28   0.003494  \n",
              "13    0.003614  84.00   0.003330  84.28   0.003497  \n",
              "11    0.002213  83.77   0.002427  84.06   0.002500  \n",
              "10    0.002213  83.77   0.002427  84.06   0.002500  \n",
              "2     0.002234  83.76   0.002540  84.05   0.002564  \n",
              "6     0.002405  83.77   0.002616  84.05   0.002696  \n",
              "7     0.002405  83.77   0.002616  84.05   0.002696  \n",
              "12    0.003720  83.43   0.003560  83.71   0.003181  \n",
              "8     0.003744  83.43   0.003578  83.71   0.003206  \n",
              "5     0.003698  83.44   0.003544  83.71   0.003160  \n",
              "4     0.003820  83.00   0.003366  83.28   0.003314  \n",
              "3     0.003820  83.00   0.003366  83.28   0.003314  \n",
              "1     0.003236  82.93   0.002952  83.23   0.003043  \n",
              "0     0.003236  82.93   0.002952  83.23   0.003043  \n",
              "14    0.002181  82.93   0.001666  83.21   0.001586  \n",
              "15    0.002181  82.93   0.001666  83.21   0.001586  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if LOAD_SCORES:\n",
        "    with open(\"outputs/scores.json\", 'r') as fd:\n",
        "        dict_scores = json.load(fd)\n",
        "\n",
        "show_results(dict_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"outputs/scores.json\",\"w\") as fd:\n",
        "    json.dump(dict_scores, fd)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EvaluationModels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
